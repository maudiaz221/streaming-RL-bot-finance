# AWS Services Setup Guide for Real-Time Stock Market Streaming Platform

## Overview
This guide will be generated by Cursor AI. It should provide step-by-step instructions for setting up all required AWS services for the streaming platform.

---

## INSTRUCTIONS FOR CURSOR

Generate a comprehensive guide covering the following sections with EXACT steps:

## 1. Prerequisites
- [ ] AWS Account with billing enabled
- [ ] AWS CLI installed and configured
- [ ] IAM user with administrative access
- [ ] AWS region selected (recommend us-east-1 for this guide)

## 2. IAM Roles and Policies Setup

### 2.1 Lambda Execution Role
**Purpose**: Allows Lambda to write to Kinesis and CloudWatch Logs

**Steps**:
1. Go to IAM Console → Roles → Create Role
2. Select trusted entity type: [SPECIFY]
3. Attach policies: [LIST EXACT POLICY NAMES OR JSON]
4. Role name: [SUGGEST NAME]
5. [ANY ADDITIONAL CONFIGURATIONS]

**Required Policy JSON**: [PROVIDE COMPLETE JSON]

### 2.2 EMR Service Role
**Purpose**: Allows EMR to manage cluster resources

**Steps**: [PROVIDE DETAILED STEPS]

### 2.3 EMR EC2 Instance Profile
**Purpose**: Allows EC2 instances in EMR cluster to access S3, Kinesis

**Steps**: [PROVIDE DETAILED STEPS]

**Required Policies**: [LIST AND PROVIDE JSON]

## 3. Amazon S3 Buckets Setup

### 3.1 Create Main Data Bucket
**Console Steps**:
1. Navigate to S3 Console
2. Click "Create bucket"
3. Bucket name: [SUGGEST NAMING CONVENTION]
4. Region: [SPECIFY]
5. Block Public Access: [ENABLE/DISABLE WITH EXPLANATION]
6. Versioning: [ENABLE/DISABLE]
7. Encryption: [SPECIFY TYPE]
8. [ANY OTHER CONFIGURATIONS]

### 3.2 Create Bucket Folders
Required folder structure:
```
[PROVIDE STRUCTURE]
```

### 3.3 Bucket Policies
[PROVIDE EXACT BUCKET POLICY JSON IF NEEDED]

## 4. Amazon Kinesis Data Streams Setup

### 4.1 Create Kinesis Data Stream
**Console Steps**:
1. Navigate to Kinesis Console
2. Select "Data Streams"
3. Click "Create data stream"
4. Data stream name: [SUGGEST NAME, e.g., "stock-market-stream"]
5. Capacity mode: 
   - [ ] On-demand (recommended for variable workloads)
   - [ ] Provisioned: [SPECIFY NUMBER OF SHARDS AND REASONING]
6. Data retention period: [RECOMMEND VALUE IN HOURS]
7. Encryption: [ENABLE/DISABLE]
8. [ANY OTHER CONFIGURATIONS]

**Cost Estimation**: [PROVIDE APPROXIMATE MONTHLY COST]

### 4.2 Testing the Stream
[PROVIDE AWS CLI COMMANDS TO TEST]
```bash
# Example command to put a test record
[PROVIDE COMMAND]
```

## 5. AWS Lambda Functions Setup

### 5.1 Create Stock Data Producer Lambda

**Console Steps**:
1. Navigate to Lambda Console
2. Click "Create function"
3. Function name: [SUGGEST NAME, e.g., "stock-data-producer"]
4. Runtime: Python 3.11
5. Execution role: [SELECT THE ROLE CREATED IN STEP 2.1]
6. Advanced settings:
   - Memory: [RECOMMEND MB]
   - Timeout: [RECOMMEND SECONDS]
   - Environment variables: [LIST REQUIRED VARIABLES]

### 5.2 Lambda Configuration
**Environment Variables**:
```
KINESIS_STREAM_NAME=[VALUE]
STOCK_API_KEY=[USER_PROVIDED]
REGION=[VALUE]
[ANY OTHERS]
```

**Layers** (if needed): [SPECIFY]

### 5.3 Set Up EventBridge Trigger
[PROVIDE STEPS TO CREATE SCHEDULED TRIGGER]
- Rate expression: [RECOMMEND, e.g., "rate(1 minute)"]

### 5.4 Deploy Lambda Code
[PROVIDE EXACT STEPS TO ZIP AND UPLOAD CODE OR USE AWS CLI]

## 6. Amazon EMR Cluster Setup

### 6.1 Create EMR Cluster for PySpark Streaming

**Console Steps**:
1. Navigate to EMR Console
2. Click "Create cluster"
3. Cluster name: [SUGGEST NAME]
4. EMR release: [SPECIFY VERSION, e.g., emr-6.15.0]
5. Applications: 
   - [X] Spark
   - [X] Hadoop
   - [ ] [ANY OTHERS?]
6. Instance configuration:
   - **Master node**: [INSTANCE TYPE, e.g., m5.xlarge]
   - **Core nodes**: [INSTANCE TYPE AND COUNT]
   - **Task nodes**: [OPTIONAL, INSTANCE TYPE AND COUNT]
7. Cluster scaling: [AUTO-SCALING RECOMMENDATIONS]
8. Networking:
   - VPC: [DEFAULT OR CUSTOM?]
   - Subnet: [SPECIFY]
   - Security groups: [DEFAULT OR CUSTOM?]
9. Bootstrap actions: [IF ANY NEEDED FOR DEPENDENCIES]
10. EC2 key pair: [USER MUST SELECT THEIR OWN]
11. IAM roles:
    - Service role: [FROM STEP 2.2]
    - Instance profile: [FROM STEP 2.3]

**Important Configurations**:
```json
[PROVIDE SPARK CONFIGURATION OVERRIDES IF NEEDED]
```

### 6.2 SSH Access to Master Node
[PROVIDE EXACT STEPS TO SET UP SSH TUNNEL FOR SPARK UI]

**Command**:
```bash
ssh -i /path/to/key.pem -L 8157:localhost:8088 hadoop@[MASTER-PUBLIC-DNS]
# Spark UI will be at localhost:4040 or 8088
```

### 6.3 Submit PySpark Streaming Job

**Using EMR Console (Steps)**:
[PROVIDE STEPS]

**Using AWS CLI**:
```bash
aws emr add-steps \
  --cluster-id [CLUSTER-ID] \
  --steps Type=Spark,Name="Stock Streaming",ActionOnFailure=CONTINUE,Args=[...]
```
[PROVIDE COMPLETE COMMAND WITH ALL ARGUMENTS]

**Using spark-submit on EMR master node**:
```bash
spark-submit \
  --packages [REQUIRED PACKAGES] \
  --conf [IMPORTANT CONFIGS] \
  s3://your-bucket/scripts/spark_streaming.py
```
[PROVIDE COMPLETE COMMAND]

## 7. Accessing Spark UI

### 7.1 Local Access (if running locally)
```
http://localhost:4040
```

### 7.2 Remote Access via SSH Tunnel (EMR)

**Setup SSH tunnel**:
```bash
[PROVIDE EXACT COMMAND]
```

**Access URLs**:
- Spark UI: [PROVIDE URL]
- Spark History Server: [PROVIDE URL]
- YARN ResourceManager: [PROVIDE URL]

## 8. Next.js Dashboard Deployment

### 8.1 Option A: Deploy to AWS Amplify
[PROVIDE STEPS IF RECOMMENDING THIS]

### 8.2 Option B: Deploy to EC2
[PROVIDE STEPS]

### 8.3 Option C: Deploy to Vercel (Recommended)
[PROVIDE STEPS AND AWS INTEGRATION]

### 8.4 Connecting Dashboard to AWS Resources
**API Gateway Setup** (if needed):
[PROVIDE STEPS]

**Environment Variables for Next.js**:
```env
AWS_REGION=[VALUE]
AWS_ACCESS_KEY_ID=[USER_PROVIDED]
AWS_SECRET_ACCESS_KEY=[USER_PROVIDED]
S3_BUCKET_NAME=[VALUE]
[OTHERS]
```

## 9. Monitoring and Logging

### 9.1 CloudWatch Logs Setup
[EXPLAIN WHERE LOGS WILL APPEAR]
- Lambda logs: [LOCATION]
- EMR logs: [LOCATION]
- Application logs: [LOCATION]

### 9.2 CloudWatch Metrics to Monitor
[LIST KEY METRICS]

### 9.3 Setting Up Alarms
[PROVIDE STEPS FOR CRITICAL ALARMS]

## 10. Cost Optimization

### 10.1 Cost Breakdown (Estimated)
```
Kinesis Data Streams: $[ESTIMATE]/month
Lambda: $[ESTIMATE]/month
EMR: $[ESTIMATE]/month (based on [X] hours/day)
S3: $[ESTIMATE]/month
[OTHERS]

TOTAL: $[ESTIMATE]/month
```

### 10.2 Cost-Saving Tips
1. [TIP 1]
2. [TIP 2]
3. Use Spot Instances for EMR task nodes
4. [MORE TIPS]

### 10.3 Cleanup Checklist
When done with the project:
- [ ] Terminate EMR cluster
- [ ] Delete Kinesis stream
- [ ] Delete Lambda functions
- [ ] Empty and delete S3 buckets
- [ ] Remove IAM roles and policies
- [ ] [ANY OTHER RESOURCES]

## 11. Security Best Practices

1. **Never commit AWS credentials to Git**
2. Use IAM roles instead of access keys when possible
3. Enable MFA on root account
4. [MORE SECURITY PRACTICES]

## 12. Troubleshooting Common Issues

### Issue 1: Lambda can't write to Kinesis
**Symptoms**: [DESCRIBE]
**Solution**: [PROVIDE SOLUTION]

### Issue 2: EMR cluster fails to start
**Symptoms**: [DESCRIBE]
**Solutions**: [LIST SOLUTIONS]

### Issue 3: PySpark job can't read from Kinesis
**Symptoms**: [DESCRIBE]
**Solution**: [PROVIDE SOLUTION]

### Issue 4: Spark UI shows high Spill (Memory/Disk)
**Symptoms**: [DESCRIBE]
**Solution**: [PROVIDE SPARK CONFIGURATION CHANGES]

[ADD MORE COMMON ISSUES]

## 13. Testing the Complete Pipeline

### End-to-End Test Steps:
1. Verify Lambda is running and pushing data to Kinesis
   ```bash
   [PROVIDE AWS CLI COMMAND TO CHECK]
   ```

2. Check Kinesis stream metrics
   ```bash
   [PROVIDE COMMAND]
   ```

3. Verify PySpark job is processing data
   - Access Spark UI
   - Check [SPECIFIC METRICS]

4. Verify data is being written to S3
   ```bash
   aws s3 ls s3://[BUCKET]/[PATH]
   ```

5. Test RL model inference
   [PROVIDE STEPS]

6. Verify dashboard is receiving data
   [PROVIDE STEPS]

## 14. Architecture Diagram
```
[CURSOR SHOULD GENERATE AN ASCII DIAGRAM OR PROVIDE DESCRIPTION]

Stock APIs → Lambda Function → Kinesis Data Stream → EMR (PySpark) → S3
                                                            ↓
                                                       RL Model
                                                            ↓
                                                    Next.js Dashboard
```

## 15. Quick Start Commands Reference

```bash
# Create Kinesis stream
aws kinesis create-stream --stream-name [NAME] --shard-count [COUNT]

# Deploy Lambda function
aws lambda create-function --function-name [NAME] [OPTIONS]

# Create EMR cluster
aws emr create-cluster [OPTIONS]

# Submit Spark job
aws emr add-steps [OPTIONS]

# List EMR clusters
aws emr list-clusters --active

# Terminate EMR cluster
aws emr terminate-clusters --cluster-ids [ID]
```

---

## Notes for Implementation
- All placeholder values in [BRACKETS] should be replaced with actual recommendations
- Include actual AWS region and availability zone recommendations
- Provide complete IAM policy JSON documents
- Include actual instance type recommendations with reasoning
- Provide realistic cost estimates based on typical usage
- Include all necessary AWS CLI commands with complete syntax